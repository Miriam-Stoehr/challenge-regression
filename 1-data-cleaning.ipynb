{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImmoEliza Project - Part 3: Regression \n",
    "\n",
    "# Step 1: Data Cleaning & Preprocessing\n",
    "\n",
    "Handling of duplicates, missing values, '0', reduction of categories, encoding etc. before feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remove Unused Encoding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataCleaner class with various methods for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the DataCleaner class with a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe to be cleaned.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def handle_missing_values(self, column: str, strategy: str = 'fill', fill_value: str = 'Unknown') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handles missing values for a specific column. By default, it fills missing values with a placeholder.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column name to handle missing values.\n",
    "            strategy (str): The strategy to handle missing values ('fill' or 'drop').\n",
    "            fill_value (str): The value to use for filling missing values if strategy is 'fill'.\n",
    "        \"\"\"\n",
    "        if strategy == 'fill':\n",
    "            self.df.loc[:, column] = self.df[column].fillna(fill_value)\n",
    "        elif strategy == 'drop':\n",
    "            self.df.dropna(subset=[column], inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be 'fill' or 'drop'.\")\n",
    "        return self.df\n",
    "    \n",
    "    def convert_dtype(self, column: str, conv_type: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts values in a column to another data type.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column name to be converted.\n",
    "            conv_type (str): The data type the values in the column should be converted to.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Pandas DataFrame with the converted column.\n",
    "        \"\"\"\n",
    "        self.df[column] = self.df[column].astype(conv_type)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def fill_missing_with_mode(self, column: str, strategy: str = 'fill') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handles missing values for a specific column. By filling them with the mode.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column name to handle missing values.\n",
    "            strategy (str): The strategy to handle missing values ('fill' or 'drop').\n",
    "        \"\"\"\n",
    "        if strategy == 'fill':\n",
    "            mode_value = self.df[column].mode()[0]\n",
    "            self.df.loc[:, column] = self.df[column].fillna(mode_value)\n",
    "        elif strategy == 'drop':\n",
    "            self.df.dropna(subset=[column], inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be 'fill' or 'drop'.\")\n",
    "        return self.df\n",
    "\n",
    "    def fill_missing_by_group(self, column: str, group_column: str, agg_func: str = 'median') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fills missing values in a column based on a group-specific statistic (e.g., median, mean).\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column with missing values to be filled.\n",
    "            group_column (str): The column to group by (e.g., 'subtype_of_property').\n",
    "            agg_func (str): The aggregation function to use ('median', 'mean', etc.).\n",
    "        \"\"\"\n",
    "        if agg_func == 'median':\n",
    "            fill_values = self.df.groupby(group_column)[column].median()\n",
    "        elif agg_func == 'mean':\n",
    "            fill_values = self.df.groupby(group_column)[column].mean()\n",
    "        else:\n",
    "            raise ValueError(\"Aggregation function must be 'median' or 'mean'.\")\n",
    "        \n",
    "        # Fill missing values with the group-specific statistic\n",
    "        self.df.loc[:, column] = self.df.apply(\n",
    "            lambda row: fill_values[row[group_column]] if pd.isnull(row[column]) else row[column],\n",
    "            axis=1\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    def drop_outliers(self, column: str, lower_bound: float = None, upper_bound: float = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drops rows where the values of a specified column are outside of the given bounds.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to filter outliers from.\n",
    "            lower_bound (float): The lower bound for valid values.\n",
    "            upper_bound (float): The upper bound for valid values.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the column is of type float64 and convert to int\n",
    "        if self.df[column].dtype == 'float64':\n",
    "            self.df.loc[:, column] = self.df[column].astype('int')\n",
    "\n",
    "        if lower_bound is not None:\n",
    "            self.df = self.df[self.df[column] >= lower_bound]\n",
    "        if upper_bound is not None:\n",
    "            self.df = self.df[self.df[column] <= upper_bound]\n",
    "        return self.df\n",
    "\n",
    "    def remove_substring(self, column:str, substring: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Removes a specified substring from all values in a given column.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column from which to remove the substring.\n",
    "            substring (str): Substring to be removed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataframe with the substring removed from the column.\n",
    "        \"\"\"\n",
    "        # Replace occurrences of the substring with an empty string\n",
    "        self.df.loc[:, column] = self.df[column].str.replace(substring, \"\", regex=False)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def binarize_column(self, column: str, threshold: float = 0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a specified column to binary values based on a threshold.\n",
    "        Values greater than the threshold are set to 1, otherwise 0.\n",
    "        \n",
    "        Args:\n",
    "            column: The column to binarize.\n",
    "            threshold: The threshold to determine binary classification.\n",
    "        \"\"\"\n",
    "        self.df.loc[:, column] = (self.df[column] > threshold).astype(int)\n",
    "        return self.df\n",
    "\n",
    "    def get_cleaned_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the cleaned dataframe.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The cleaned dataframe.\n",
    "        \"\"\"\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused part of DataCleaner\n",
    "def replace_smaller_value(self, column: str, condition_column: str):\n",
    "    \"\"\"\n",
    "    Replaces values in the 'column' with values from the 'condition_column' when a condition is met.\n",
    "    E.g., if the value in 'condition_column' is greater than the value in 'column', replace the value in 'column' with the value from 'condition_column'.\n",
    "    \n",
    "    Args:\n",
    "        column: The column to modify.\n",
    "        condition_column: The column whose value will be used for replacement.\n",
    "    \"\"\"\n",
    "    self.df.loc[:, column] = self.df.apply(\n",
    "        lambda row: row[condition_column] if row[condition_column] > row[column] else row[column], axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataEncoder class with various methods for data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "class DataEncoder:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the DataEncoder class with a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe to be encoded.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def standardize_categories(self, column: str, mapping: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Standardizes categorical values based on a given mapping.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to standardize.\n",
    "            mapping (dict): A dictionary that maps old values to new values.\n",
    "        \"\"\"\n",
    "        self.df.loc[:, column] = self.df.loc[:, column].map(mapping)\n",
    "        return self.df\n",
    "    \n",
    "    def manual_mapping(self, column: str, mapping: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Applies manual mapping to a specified column using the provided dictionary.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to map.\n",
    "            mapping (dict): A dictionary defining the mapping of the column's values.\n",
    "        \"\"\"\n",
    "        # Ensure the column exists\n",
    "        if column not in self.df.columns:\n",
    "            raise KeyError(f\"Column '{column}' not found in DataFrame.\")\n",
    "        # Apply the mapping\n",
    "        self.df.loc[:, column] = self.df.loc[:, column].map(mapping)\n",
    "        return self.df\n",
    "\n",
    "    def encode_label(self, column: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encodes a categorical column using label encoding.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to encode.\n",
    "        \"\"\"\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df.loc[:, column] = label_encoder.fit_transform(self.df[column])\n",
    "        return self.df\n",
    "\n",
    "    def encode_onehot(self, column: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encodes a categorical column using one-hot encoding.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to encode.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The original dataframe with new one-hot encoded columns.\n",
    "        \"\"\"\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        encoded_data = onehot_encoder.fit_transform(self.df[[column]])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=onehot_encoder.categories_[0])\n",
    "        encoded_df = encoded_df.astype(int) # Convert boolean to integers\n",
    "        self.df = pd.concat([self.df, encoded_df], axis=1)\n",
    "        self.df.drop(columns=[column], inplace=True)  # Drop original column after encoding\n",
    "        return self.df\n",
    "\n",
    "    def encode_dummy(self, column: str, prefix: str = \"\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encodes a categorical column using dummy encoding.\n",
    "\n",
    "        Args:\n",
    "            column (str): The column to encode.\n",
    "            prefix (str): The prefix to add to the new column names.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The original dataframe with dummy encoded columns.\n",
    "        \"\"\"\n",
    "        dummy_df = pd.get_dummies(self.df[column], drop_first=True, prefix=prefix) # avoids multicollinearity\n",
    "        dummy_df = dummy_df.astype(int) # Convert boolean columns to integers\n",
    "        self.df = pd.concat([self.df, dummy_df], axis=1)\n",
    "        self.df.drop(columns=[column], inplace=True) # Drop original column after encoding\n",
    "        return self.df\n",
    "\n",
    "    def get_encoded_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the dataframe with encoded categorical variables.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The dataframe with encoded columns.\n",
    "        \"\"\"\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"./data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values & Outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataCleaner instance\n",
    "cleaner = DataCleaner(df)\n",
    "\n",
    "# Drop missing values in 'price' column\n",
    "df_cleaned = cleaner.handle_missing_values('price', strategy='drop')\n",
    "\n",
    "# Fill missing 'facade_number' based on median per 'subtype_of_property'\n",
    "df_cleaned = cleaner.fill_missing_by_group('facade_number', group_column='subtype_of_property', agg_func='median')\n",
    "\n",
    "# Drop outliers in 'living_area' (<12), 'facade_number' (>6), bedroom_nr (>24) -> since compared to other values of these data points, these entries don't seem realistic\n",
    "df_cleaned = cleaner.drop_outliers('living_area', lower_bound=12)\n",
    "df_cleaned = cleaner.drop_outliers('facade_number', upper_bound=6.0)\n",
    "df_cleaned = cleaner.drop_outliers('bedroom_nr', upper_bound=24)\n",
    "\n",
    "# Remove substring 'unit' from \"subtype_of_property\"\n",
    "df_cleaned = cleaner.remove_substring(\"subtype_of_property\", \" unit\")\n",
    "\n",
    "# Fill missing 'building_condition' with mode\n",
    "df_cleaned = cleaner.fill_missing_with_mode('building_condition', strategy='fill')\n",
    "\n",
    "# Convert 'facade_number' to int\n",
    "df_cleaned = cleaner.convert_dtype('facade_number', 'int')\n",
    "\n",
    "# Change 'terrace' to binary (if value: 1, if not, 0)\n",
    "df_cleaned = cleaner.binarize_column('terrace', threshold=0)\n",
    "\n",
    "# Get cleaned dataframe\n",
    "df_cleaned = cleaner.get_cleaned_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataEncoder instance\n",
    "encoder = DataEncoder(df_cleaned)\n",
    "\n",
    "# Data Encoding: Standardize 'equipped_kitchen' categories\n",
    "df_encoded = encoder.standardize_categories('equipped_kitchen', {\n",
    "    'installed': 'installed',\n",
    "    'semi equipped': 'semi equipped',\n",
    "    'hyper equipped': 'hyper equipped',\n",
    "    'not installed': 'not installed',\n",
    "    'usa installed': 'installed',\n",
    "    'usa hyper equipped': 'hyper equipped',\n",
    "    'usa semi equipped': 'semi equipped',\n",
    "    'usa uninstalled': 'not installed',\n",
    "    '0': 'not installed'\n",
    "})\n",
    "\n",
    "# Apply manual mapping to \"equipped_kitchen\" column\n",
    "kitchen_mapping = {\n",
    "    'not installed': 0,\n",
    "    'semi equipped': 1,\n",
    "    'installed': 2,\n",
    "    'hyper equipped': 3\n",
    "}\n",
    "\n",
    "df_encoded = encoder.manual_mapping('equipped_kitchen', kitchen_mapping)\n",
    "\n",
    "# Apply dummy encoding to equipped kitchen\n",
    "#df_encoded = encoder.encode_dummy('equipped_kitchen', prefix='kitchen')\n",
    "\n",
    "# Apply manual mapping to \"building_condition\" column -> start at 1, since 0 reserved for 'not present'\n",
    "condition_mapping = {\n",
    "    'as new': 6,\n",
    "    'just renovated': 5,\n",
    "    'good': 4,\n",
    "    'to be done up': 3,\n",
    "    'to renovate': 2,\n",
    "    'to restore': 1\n",
    "}\n",
    "\n",
    "df_encoded = encoder.manual_mapping('building_condition', condition_mapping)\n",
    "\n",
    "# Apply dummy encoding to building condition\n",
    "#df_encoded = encoder.encode_dummy('building_condition', prefix='condition')\n",
    "\n",
    "# Apply manual mapping to \"subtype_of_property\" column to reduce categories\n",
    "\n",
    "subtype_categories = {\n",
    "    'kot': 'apartment',\n",
    "    'chalet': 'house',\n",
    "    'flat studio': 'apartment',\n",
    "    'service flat': 'apartment',\n",
    "    'bungalow': 'house',\n",
    "    'town house': 'house',\n",
    "    'ground floor': 'apartment',\n",
    "    'apartment': 'apartment',\n",
    "    'house': 'house',\n",
    "    'mixed use building': 'mixed use building',\n",
    "    'triplex': 'house',\n",
    "    'farmhouse': 'mixed use building',\n",
    "    'loft': 'apartment',\n",
    "    'duplex': 'house',\n",
    "    'apartment block': 'mixed use building',\n",
    "    'country cottage': 'house',\n",
    "    'penthouse': 'luxury',\n",
    "    'mansion': 'luxury',\n",
    "    'other property': 'other',\n",
    "    'villa': 'luxury',\n",
    "    'exceptional property': 'luxury',\n",
    "    'manor house': 'luxury',\n",
    "    'castle': 'luxury'\n",
    "}\n",
    "\n",
    "df_encoded = encoder.manual_mapping('subtype_of_property', subtype_categories)\n",
    "\n",
    "# Apply manual mapping to reduced categories in \"subtypes of property\"\n",
    "\n",
    "subtype_mapping = {\n",
    "    'apartment':1,\n",
    "    'house': 2,\n",
    "    'mixed use building': 3,\n",
    "    'other': 4,\n",
    "    'luxury': 5\n",
    "}\n",
    "\n",
    "df_encoded = encoder.manual_mapping('subtype_of_property', subtype_mapping)\n",
    "\n",
    "# Apply dummy encoding to subtype\n",
    "#df_encoded = encoder.encode_dummy('subtype_of_property', prefix='subtype')\n",
    "\n",
    "# Get final encoded dataframe\n",
    "df_encoded = encoder.get_encoded_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataframe\n",
    "df_encoded.to_csv('./data/1_cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
